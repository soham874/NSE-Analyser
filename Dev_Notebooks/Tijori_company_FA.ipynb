{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bd0f6-ec89-4a02-a8bc-153d8a958905",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_company = 'polycab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93242cc-5281-4fa6-93ef-c25cb62e2ab3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# company search\n",
    "import requests\n",
    "import json\n",
    "\n",
    "headers = headers_dict = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.request(\n",
    "    \"GET\", \n",
    "    url = f\"https://www.tijorifinance.com/api/v1/ind/company_search/?q={search_company}\", \n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "search_results = []\n",
    "slug = ''\n",
    "\n",
    "if not 200 <= response.status_code < 400:\n",
    "    print(response.text)\n",
    "else:\n",
    "    for item in json.loads(response.text):\n",
    "        if item['type'] == 'companies':\n",
    "            search_results.append(\n",
    "                {\n",
    "                    'name' : item['name'],\n",
    "                    'slug' : item['slug']\n",
    "                }\n",
    "            )\n",
    "    if len(search_results) > 1:\n",
    "        print(\"Update appropriate slug in next cell\")\n",
    "        print(json.dumps(search_results, ensure_ascii=False, indent=4))\n",
    "    elif len(search_results) == 1:\n",
    "        slug = search_results[0]['slug']\n",
    "        print(\"Single slug found and loaded, please skip next cell and proceed\")\n",
    "    else:\n",
    "        print(\"No matching company found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66e36e-1f78-4109-bf0a-7dac77ef8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = 'polycab-india-ltd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15284942-c19f-4906-b2ca-49d1b29294e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fetch particular company info\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "headers = headers_dict = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "soup_overview = None\n",
    "soup_benchmarking = None\n",
    "soup_financials = None\n",
    "soup_shareholding = None\n",
    "\n",
    "response_overview = requests.request(\n",
    "    \"GET\", \n",
    "    url = f\"https://www.tijorifinance.com/company/{slug}\", \n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if not 200 <= response_overview.status_code < 400:\n",
    "    print(response_overview.text)\n",
    "else:\n",
    "    print(f\"Success fetch overview - response_overview -> https://www.tijorifinance.com/company/{slug}\")\n",
    "    soup_overview = BeautifulSoup(response_overview.text, 'html.parser')\n",
    "\n",
    "response_financials = requests.request(\n",
    "    \"GET\", \n",
    "    url = f\"https://www.tijorifinance.com/company/{slug}/financials/\", \n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if not 200 <= response_financials.status_code < 400:\n",
    "    print(response_financials.text)\n",
    "else:\n",
    "    print(f\"Success fetch financials - response_financials -> https://www.tijorifinance.com/company/{slug}/financials/\")\n",
    "    soup_financials = BeautifulSoup(response_financials.text, 'html.parser')\n",
    "\n",
    "response_benchmarking = requests.request(\n",
    "    \"GET\", \n",
    "    url = f\"https://www.tijorifinance.com/company/{slug}/benchmarking/\", \n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if not 200 <= response_benchmarking.status_code < 400:\n",
    "    print(response_benchmarking.text)\n",
    "else:\n",
    "    print(f\"Success fetch benchamarking - response_benchmarking -> https://www.tijorifinance.com/company/{slug}/benchmarking/\")\n",
    "    soup_benchmarking = BeautifulSoup(response_benchmarking.text, 'html.parser')\n",
    "    \n",
    "response_shareholding = requests.request(\n",
    "    \"GET\", \n",
    "    url = f\"https://www.tijorifinance.com/company/{slug}/shareholding/\", \n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if not 200 <= response_shareholding.status_code < 400:\n",
    "    print(response_shareholding.text)\n",
    "else:\n",
    "    print(f\"Success fetch shareholding - response_shareholding -> https://www.tijorifinance.com/company/{slug}/shareholding/\")\n",
    "    soup_shareholding = BeautifulSoup(response_shareholding.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d952dca-5923-4e03-b4b4-c2d504ab04fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get about, market cap, pe ratio\n",
    "\n",
    "data = {}\n",
    "data['summary'] = soup_overview.find('div', class_='about_desc detailed_desc').text.strip().split('\\n')[0].strip()\n",
    "data['market_cap'] = (int((soup_overview.find('div', string='Market Cap').find_next('div', class_='company_details_value').text).replace('â‚¹', '').replace(' Cr.', '').replace(',', '')) * 10**7 )\n",
    "data['pe_ratio'] = float(soup_overview.find('div', string='P/E').find_next('div', class_='company_details_value').text)\n",
    "\n",
    "\n",
    "print(json.dumps(data, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309e97c-89ea-406e-8da2-2574e754623b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get all FAQs list\n",
    "faq_data = []\n",
    "\n",
    "for soup in [soup_overview, soup_financials, soup_benchmarking, soup_shareholding]:\n",
    "    \n",
    "    faq_containers = soup.find_all('div', class_='faq_container')\n",
    "    \n",
    "    for container in faq_containers:\n",
    "        question = container.find('div', class_='faq_data_header').find('span').text.strip()\n",
    "        answer = container.find('div', class_='faq_answer').find('p').text.strip()\n",
    "        \n",
    "        faq_data.append(\n",
    "            {\n",
    "                'question': question,\n",
    "                'answer': answer.split(\"\\n\")[0]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "print(json.dumps(faq_data, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2b9c8-e4bf-4d4a-b2e7-4ea5998c9100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get company forensics ( 1 - good, 2 - neutral, 3 - bad, 4 - not calculated )\n",
    "\n",
    "script_data = soup_overview.find('script', id='company_details_data', type='application/json').text.strip()\n",
    "forensics_data = json.loads(script_data)\n",
    "\n",
    "if forensics_data['quick_look']:\n",
    "    # Collect the factories\n",
    "    forensics = forensics_data['quick_look']['data'][0]['factories']\n",
    "    forensics.extend(forensics_data['quick_look']['data'][1]['factories'])\n",
    "    \n",
    "    forensics = sorted(forensics, key=lambda x: x['flag'], reverse=True)\n",
    "    \n",
    "    print(json.dumps(forensics, ensure_ascii=False, indent=4))\n",
    "else:\n",
    "    print('No forensics data available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4b58e-59a7-4e1b-96cb-950967eb9900",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Peers data for comparision\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_scrollable_dataframe(df_pivot, table_height = 150):\n",
    "    html = df_pivot.to_html(classes='table table-bordered', index=False)\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"height:{table_height}px; overflow:auto; border:1px solid lightgrey; padding:5px;\">\n",
    "        {html}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "# soup = BeautifulSoup(response_overview.text, 'html.parser')\n",
    "\n",
    "script_data = soup_overview.find('script', id='peers_table_data', type='application/json').text.strip()\n",
    "peers_data = json.loads(script_data)\n",
    "\n",
    "df_peers_data = pd.DataFrame(peers_data[1]['data'])\n",
    "\n",
    "display_scrollable_dataframe(df_peers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb1c91-e304-488e-85c4-f0787f3c1792",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shareholding data over the years\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for script_tag in soup_shareholding.find_all('script'):\n",
    "    js_code = script_tag.string\n",
    "    if js_code: \n",
    "        try:\n",
    "            if 'var trendData' in js_code or 'var trendData2' in js_code:\n",
    "                start_index = js_code.index('[') \n",
    "                end_index = js_code.rindex(']') + 1\n",
    "\n",
    "                json_string = js_code[start_index:end_index]\n",
    "                data = json.loads(json_string.replace(\"'\", '\"')) \n",
    "\n",
    "                all_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting JSON from script: {e}\")\n",
    "\n",
    "flattened_data = []\n",
    "for dataset in all_data:\n",
    "    for entry in dataset:\n",
    "        name = entry['name']\n",
    "        for item in entry['data']:\n",
    "            timestamp = item[0]  # The timestamp\n",
    "            value = item[1]      # The associated value\n",
    "            flattened_data.append({'Name': name, 'Timestamp': timestamp, 'Value': value})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df['DateIST'] = pd.to_datetime(df['Timestamp'], unit='ms') + pd.Timedelta(hours=5, minutes=30)\n",
    "df['DateIST'] = df['DateIST'].dt.strftime('%Y-%m')\n",
    "\n",
    "df_shareholder_info = df.pivot_table(index='Name', columns='DateIST', values='Value')\n",
    "df_shareholder_info.reset_index(inplace=True)\n",
    "\n",
    "display_scrollable_dataframe(df_shareholder_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0594c2e-bde5-49ea-9459-3443c2e7c8e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Financial data\n",
    "\n",
    "script_data = soup_financials.find('script', id='fin_tables_data', type='application/json').text.strip()\n",
    "fin_data = json.loads(script_data)\n",
    "\n",
    "def process_section(report_dates,entry_list,name = 'Overall'): # pass list\n",
    "\n",
    "    #report_dates.sort(reverse = True)\n",
    "\n",
    "    values_dict = {date: [] for date in report_dates}  # Dictionary to hold values for each report date\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each subsection\n",
    "    for section in entry_list:\n",
    "        sub_section_name = section['name']  # Category name\n",
    "        \n",
    "        # Add the values for each report date\n",
    "        for i, date in enumerate(report_dates):\n",
    "            if i < len(section['value']) and section['value'][i]:  \n",
    "                values_dict[date].append(round(section['value'][i],2))\n",
    "            else:\n",
    "                values_dict[date].append(0)\n",
    "\n",
    "        df = pd.concat(\n",
    "                [df,pd.DataFrame({\n",
    "                    'category': sub_section_name,\n",
    "                    **values_dict \n",
    "                })],\n",
    "                ignore_index = True\n",
    "            )\n",
    "\n",
    "        values_dict = {date: [] for date in report_dates}\n",
    "\n",
    "        if section['sub_section']:\n",
    "            df = pd.concat(\n",
    "                    [df,process_section(report_dates,section['sub_section'], sub_section_name)],\n",
    "                    ignore_index = True\n",
    "                )\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_complete_report(report_key = 'pl_c_d'):\n",
    "    dict_shorts_0 = {\n",
    "        'pl' : 'Profit and Loss',\n",
    "        'bs' : 'Balance Sheet',\n",
    "        'cf' : 'Cash Flow',\n",
    "        'qt' : 'Quarterly',\n",
    "        'growth' : 'Growth Table',\n",
    "        'fr' : 'Financial Ratio'\n",
    "    }\n",
    "    \n",
    "    dict_shorts_1 = {\n",
    "        'c' : 'consolidated',\n",
    "        's' : 'standalone'\n",
    "    }\n",
    "    \n",
    "    dict_shorts_2 = {\n",
    "        's' : 'summary',\n",
    "        'd' : 'detail'\n",
    "    }\n",
    "    \n",
    "    report_name = ''\n",
    "    if len(report_key.split(\"_\")) > 0:\n",
    "        report_name += dict_shorts_0[report_key.split(\"_\")[0]]+' '\n",
    "    if len(report_key.split(\"_\")) > 1:\n",
    "        if report_key.split(\"_\")[1] == 's':\n",
    "            print(f'Skipping standalone report for {report_name}')\n",
    "            return\n",
    "            \n",
    "        report_name += dict_shorts_1[report_key.split(\"_\")[1]]+' '\n",
    "    if len(report_key.split(\"_\")) > 2:\n",
    "        report_name += dict_shorts_2[report_key.split(\"_\")[2]]+' '\n",
    "        if report_key.split(\"_\")[2] == 's':\n",
    "            print(f'Skipping summary report for {report_name}')\n",
    "            return\n",
    "            \n",
    "    report_name += 'statement'\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(report_name)\n",
    "    if fin_data[report_key]:\n",
    "        display_scrollable_dataframe(\n",
    "            process_section(\n",
    "                fin_data[report_key]['report_dates'],\n",
    "                fin_data[report_key]['data'],\n",
    "                name\n",
    "            ),\n",
    "            400\n",
    "        )\n",
    "    else:\n",
    "        print(\"No data available\")\n",
    "\n",
    "for key in fin_data.keys():\n",
    "    get_complete_report(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
